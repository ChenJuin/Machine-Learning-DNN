{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d12eaac-4c07-48c3-ae6d-51491587746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# 从CSV文件加载数据\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # 跳过文件头部\n",
    "        for line in reader:\n",
    "            sentence, label = line\n",
    "            data.append((sentence, int(label == 'negative')))\n",
    "    return data\n",
    "\n",
    "# 数据集类\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# 定义自定义模型\n",
    "class ALBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ALBERTClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.neck = nn.Linear(self.albert.config.hidden_size, 256)\n",
    "        self.tail = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        features = self.neck(pooled_output)\n",
    "        logits = self.tail(features)\n",
    "        return logits\n",
    "\n",
    "# 加载数据集并进行分割\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "data = load_data('IMDB-LT.csv')\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fbfe3d6-ea3a-4033-a325-45a17024f2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcce34f9e6143098b03cb059d45bc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 创建训练集和验证集\n",
    "train_dataset = IMDbDataset(train_data, tokenizer)\n",
    "val_dataset = IMDbDataset(val_data, tokenizer)\n",
    "\n",
    "# 定义超参数\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# 构建基分类器模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = ALBERTClassifier(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "16356279-69e7-4ca8-b22b-2f9d1b7eeb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6155, Accuracy: 0.7267\n",
      "Val Loss: 0.6846, Val Accuracy: 0.9756\n",
      "Epoch 2/10, Loss: 0.3355, Accuracy: 0.9938\n",
      "Val Loss: 0.8227, Val Accuracy: 0.9756\n",
      "Epoch 3/10, Loss: 0.2809, Accuracy: 0.9938\n",
      "Val Loss: 1.0168, Val Accuracy: 0.9756\n",
      "Epoch 4/10, Loss: 0.2135, Accuracy: 0.9938\n",
      "Val Loss: 0.8745, Val Accuracy: 0.9756\n",
      "Epoch 5/10, Loss: 0.1914, Accuracy: 0.9938\n",
      "Val Loss: 1.0724, Val Accuracy: 0.9756\n",
      "Epoch 6/10, Loss: 0.1420, Accuracy: 0.9938\n",
      "Val Loss: 1.2297, Val Accuracy: 0.9756\n",
      "Epoch 7/10, Loss: 0.0863, Accuracy: 1.0000\n",
      "Val Loss: 1.4019, Val Accuracy: 0.9756\n",
      "Epoch 8/10, Loss: 0.0835, Accuracy: 1.0000\n",
      "Val Loss: 1.4277, Val Accuracy: 0.9756\n",
      "Epoch 9/10, Loss: 0.0568, Accuracy: 1.0000\n",
      "Val Loss: 1.5323, Val Accuracy: 0.9756\n",
      "Epoch 10/10, Loss: 0.0468, Accuracy: 1.0000\n",
      "Val Loss: 1.6105, Val Accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# 创建装袋模型\n",
    "ensemble_model = BaggingClassifier(base_model, n_estimators=10, random_state=42)\n",
    "\n",
    "# 定义损失函数\n",
    "class_weights = torch.tensor([1, 100], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=2e-5)\n",
    "\n",
    "# 训练模型\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epoch = 0\n",
    "max_no_improve_epoch = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    base_model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = base_model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算准确度\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # 验证步骤\n",
    "    base_model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "    all_val_labels = []\n",
    "    all_val_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            val_input_ids, val_attention_mask, val_labels = val_batch\n",
    "            val_input_ids = val_input_ids.to(device)\n",
    "            val_attention_mask = val_attention_mask.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            val_logits = base_model(val_input_ids, val_attention_mask)\n",
    "            val_loss = criterion(val_logits, val_labels)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # 计算验证集的准确度\n",
    "            val_predictions = torch.argmax(val_logits, dim=1)\n",
    "            total_val_correct += (val_predictions == val_labels).sum().item()\n",
    "            total_val_samples += val_labels.size(0)\n",
    "            all_val_predictions.extend(val_predictions.cpu().numpy())\n",
    "            all_val_labels.extend(val_labels.cpu().numpy())\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = total_val_correct / total_val_samples\n",
    "    print(f'Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 如果验证损失没有改进，增加无改进的轮数\n",
    "    if average_val_loss >= best_val_loss:\n",
    "        no_improve_epoch += 1\n",
    "    else: # 否则，保存最好的模型，并重置无改进的轮数\n",
    "        torch.save(base_model.state_dict(), 'best_model.pth')\n",
    "        best_val_loss = average_val_loss\n",
    "        no_improve_epoch = 0\n",
    "\n",
    "    # 如果无改进的轮数达到最大值，提前结束训练\n",
    "    if no_improve_epoch >= max_no_improve_epoch:\n",
    "        print(f\"Validation loss has not improved for {no_improve_epoch} epochs, stopping early.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7cba161-aab0-4a24-906a-dcff78a68150",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1849/3505210414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 训练装袋模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 生成混淆矩阵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_val_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_val_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    281\u001b[0m         X, y = check_X_y(\n\u001b[1;32m    282\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# 将预测转换为numpy数组\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# 训练装袋模型\n",
    "ensemble_model.fit(all_predictions, all_labels)\n",
    "\n",
    "# 生成混淆矩阵\n",
    "cm = confusion_matrix(all_val_labels, all_val_predictions)\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2326ee-c9ed-4e11-a984-2a286ee1862a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
